{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainerCallback, TrainingArguments, DataCollatorWithPadding\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('input/arguments-training.tsv', sep=\"\\t\")\n",
    "val_data = pd.read_csv('input/arguments-validation.tsv', sep=\"\\t\")\n",
    "train_labels = pd.read_csv('input/labels-training.tsv', sep=\"\\t\")\n",
    "val_labels = pd.read_csv('input/labels-validation.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['combined1'] = train_data.Conclusion.str.cat(train_data.Stance, sep=' [SEP] ')\n",
    "train_data['combined2'] = train_data.combined1.str.cat(train_data.Premise, sep=' [SEP] ')\n",
    "val_data['combined1'] = val_data.Conclusion.str.cat(val_data.Stance, sep=' [SEP] ')\n",
    "val_data['combined2'] = val_data.combined1.str.cat(val_data.Premise, sep=' [SEP] ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We should ban naturopathy [SEP] against [SEP] it provides a useful income for some people',\n",
       "  'We should ban fast food [SEP] in favor of [SEP] fast food should be banned because it is really bad for your health and is costly.'],\n",
       " [\"Surrogacy should be banned [SEP] against [SEP] Surrogacy should not be banned as it is the woman's right to choose if she wishes to do this for another couple and be compensated.\",\n",
       "  'Entrapment should be legalized [SEP] against [SEP] entrapment is gravely immoral and against human rights to coerce someone into a crime'],\n",
       " [['A01004', 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  ['A01005', 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [['A02002', 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  ['A02009', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1]]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = train_data.combined2.values.tolist()\n",
    "val_combined = val_data.combined2.values.tolist()\n",
    "label_list = train_labels.values.tolist()\n",
    "val_label_list = val_labels.values.tolist()\n",
    "[x[3:5] for x in [combined, val_combined, label_list, val_label_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tokens(combined, val_combined): #best to functionalize, this function is quite slow so lets not call it on every kernel restart.\n",
    "    symbols = []\n",
    "    for each in combined:\n",
    "        ids = tok(each, add_special_tokens=True, padding=\"max_length\", return_tensors='pt')\n",
    "        symbols.append(ids)\n",
    "\n",
    "    val_symbols = []\n",
    "    for each in val_combined:\n",
    "        ids = tok(each, add_special_tokens=True, padding=\"max_length\", return_tensors='pt')\n",
    "        val_symbols.append(ids)\n",
    "\n",
    "    bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "    X = []\n",
    "    for s in symbols:\n",
    "        encoded_sequence = bert(**s)\n",
    "        bert_output = encoded_sequence.pooler_output\n",
    "        output = torch.unsqueeze(bert_output,1)\n",
    "        X.append(output[0][0].tolist())\n",
    "    print(len(X),X[0])\n",
    "    X_val = []\n",
    "    for s in val_symbols:\n",
    "        encoded_sequence = bert(**s)\n",
    "        bert_output = encoded_sequence.pooler_output\n",
    "        output = torch.unsqueeze(bert_output,1)\n",
    "        X_val.append(output[0][0].tolist())\n",
    "    print(np.array(label_list).shape)\n",
    "    \n",
    "    clf = MultiOutputClassifier(LogisticRegression(max_iter=1000)).fit(X, np.array(label_list)) \n",
    "    p = clf.predict(X_val)\n",
    "    print(classification_report(val_label_list, p))\n",
    "    return p \n",
    "# gen_tokens(combined, val_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate a list of labels to feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['combined2', 'Self-direction: thought', 'Self-direction: action',\n",
       "        'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance',\n",
       "        'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
       "        'Tradition', 'Conformity: rules', 'Conformity: interpersonal',\n",
       "        'Humility', 'Benevolence: caring', 'Benevolence: dependability',\n",
       "        'Universalism: concern', 'Universalism: nature',\n",
       "        'Universalism: tolerance', 'Universalism: objectivity'],\n",
       "       dtype='object'),\n",
       " Index(['combined2', 'Self-direction: thought', 'Self-direction: action',\n",
       "        'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance',\n",
       "        'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
       "        'Tradition', 'Conformity: rules', 'Conformity: interpersonal',\n",
       "        'Humility', 'Benevolence: caring', 'Benevolence: dependability',\n",
       "        'Universalism: concern', 'Universalism: nature',\n",
       "        'Universalism: tolerance', 'Universalism: objectivity'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets generate Dataset objects so we can feed it into huggingface.Trainer\n",
    "valid_data = val_data.merge(val_labels, how = 'left', on = 'Argument ID')\n",
    "t_data = train_data.merge(train_labels, how = 'left', on = 'Argument ID')\n",
    "\n",
    "if 'Argument ID' in valid_data.columns: valid_data.drop(['Argument ID', 'Conclusion', 'Stance', 'Premise', 'combined1'], axis=1, inplace=True)\n",
    "if 'Argument ID' in t_data.columns: t_data.drop(['Argument ID', 'Conclusion', 'Stance', 'Premise', 'combined1'],axis=1, inplace=True)\n",
    "valid_data.columns, t_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity']\n"
     ]
    }
   ],
   "source": [
    "#huggingface dataset.from_pandas for ex.\n",
    "\n",
    "train_to_dataset = Dataset.from_pandas(t_data)\n",
    "val_to_dataset = Dataset.from_pandas(valid_data)\n",
    "dataset = DatasetDict({ \"train\": train_to_dataset, \"validation\": val_to_dataset})\n",
    "labels = list(valid_data.columns)\n",
    "labels = labels[1:] #ignore combined2\n",
    "print(labels)\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#from huggingface docu\n",
    "def preprocess(items):\n",
    "  combined = items['combined2']\n",
    "  encoding = tokenizer(combined, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "  l_b = {k: items[k] for k in items.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  print(len(combined))\n",
    "  labels_mat = np.zeros((len(combined), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_mat[:, idx] = l_b[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_mat.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f012fc2f08bd455491211d63502e7fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5714b6aa0b5446eba3c77377d25e6e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cop = copy.deepcopy(dataset)\n",
    "encoded_dataset = dataset_cop.map(preprocess, batched=True,remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "def compute_metrics(eval):\n",
    "    result = mlmetrics(\n",
    "        pred=eval.predictions, \n",
    "        y_true=eval.label_ids)\n",
    "    return result\n",
    "\n",
    "def mlmetrics(pred, y_true):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(pred))\n",
    "\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    #can't sklearn.metrics.classification_report as huggingface expects string:int mapping, vs string:string.\n",
    "    precision_micro_average = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_micro_average = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {'p': precision_micro_average,\n",
    "               'r': recall_micro_average,\n",
    "               'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065a94b48ce44cc58794f6f2a73ca3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050a7f5d16534a14a0b9f649ce823e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train, test_valid = t_data[0:2], valid_data[0:2]\n",
    "td = Dataset.from_pandas(test_train)\n",
    "vd = Dataset.from_pandas(test_valid)\n",
    "datasetTest = DatasetDict({ \"train\": td, \"validation\": vd})\n",
    "e_d = datasetTest.map(preprocess, batched=True,remove_columns=datasetTest['train'].column_names)\n",
    "e_d\n",
    "# datasetTest['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE, BATCH_SIZE, EPOCHS = 1e-4, 8, 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"camembert-fine-tuned\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5220\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1306\n",
      "  Number of trainable parameters = 109497620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b364eaa3e44b3b8fa997d4b79406ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3041, 'learning_rate': 6.171516079632466e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1896\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b0a84fa52c48bbaf4f167c14def8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to camembert-fine-tuned/checkpoint-653\n",
      "Configuration saved in camembert-fine-tuned/checkpoint-653/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29577159881591797, 'eval_p': 0.7581536760641239, 'eval_r': 0.43135713162446926, 'eval_f1': 0.5498646887842037, 'eval_roc_auc': 0.7018165208833668, 'eval_accuracy': 0.10917721518987342, 'eval_runtime': 76.3151, 'eval_samples_per_second': 24.844, 'eval_steps_per_second': 3.106, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in camembert-fine-tuned/checkpoint-653/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2641, 'learning_rate': 2.343032159264931e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1896\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6b72b8f00e4889af4deac1e7bb2984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to camembert-fine-tuned/checkpoint-1306\n",
      "Configuration saved in camembert-fine-tuned/checkpoint-1306/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2574668824672699, 'eval_p': 0.7941642162406696, 'eval_r': 0.5521308381821041, 'eval_f1': 0.65139146567718, 'eval_roc_auc': 0.7616488923650294, 'eval_accuracy': 0.1719409282700422, 'eval_runtime': 77.0235, 'eval_samples_per_second': 24.616, 'eval_steps_per_second': 3.077, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in camembert-fine-tuned/checkpoint-1306/pytorch_model.bin\n",
      "Deleting older checkpoint [camembert-fine-tuned/checkpoint-2] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from camembert-fine-tuned/checkpoint-1306 (score: 0.65139146567718).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1585.2675, 'train_samples_per_second': 6.586, 'train_steps_per_second': 0.824, 'train_loss': 0.2744884461757783, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1306, training_loss=0.2744884461757783, metrics={'train_runtime': 1585.2675, 'train_samples_per_second': 6.586, 'train_steps_per_second': 0.824, 'train_loss': 0.2744884461757783, 'epoch': 2.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1896\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e969d9cfaa44ce9656c193c5f3845e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2574668824672699,\n",
       " 'eval_p': 0.7941642162406696,\n",
       " 'eval_r': 0.5521308381821041,\n",
       " 'eval_f1': 0.65139146567718,\n",
       " 'eval_roc_auc': 0.7616488923650294,\n",
       " 'eval_accuracy': 0.1719409282700422,\n",
       " 'eval_runtime': 74.5473,\n",
       " 'eval_samples_per_second': 25.434,\n",
       " 'eval_steps_per_second': 3.179,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
